<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-11-01T23:49:13+11:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Julien Thurin</title><subtitle>Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description.</subtitle><entry><title type="html">Intro to Moment Tensor Inversion</title><link href="http://localhost:4000/jekyll/update/2024/10/23/Intro_to_MT_inversion.html" rel="alternate" type="text/html" title="Intro to Moment Tensor Inversion" /><published>2024-10-23T01:43:14+11:00</published><updated>2024-10-23T01:43:14+11:00</updated><id>http://localhost:4000/jekyll/update/2024/10/23/Intro_to_MT_inversion</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/10/23/Intro_to_MT_inversion.html"><![CDATA[<p>This introduction is a brief overview of the moment tensor inversion problem as handled by the <code class="language-plaintext highlighter-rouge">MTUQ</code> package. It is directly copied from the Google Colab notebook that I used for the <a href="https://seisscoped.org/workshop-2024/">SCOPED 2024 Workshop</a> session on moment tensor inversion. The notebook can be found <a href="https://colab.research.google.com/drive/1UJWOompBz9MlJN0B6SoVKzF8Whz_1nPp?usp=sharing">here</a>.</p>

<p><small>Please reach out to me if you have any issues or questions about the content of the Colab notebook.</small></p>

<hr style="border: 1px solid   #8caba1; width: 100%;" />

<!-- vertical space -->
<div style="height: 30px;"></div>

<h1 id="introduction-to-moment-tensor-inversion-with-the-cut-and-paste-method">Introduction to Moment tensor inversion with the Cut-and-Paste method</h1>

<p>In this brief introduction, we will see what the moment tensor inversion problem is about. This should cover all the theory you should be familiar with before handling MTUQ, starting with:</p>

<h3 id="the-seismic-moment-tensor">The seismic moment tensor</h3>

<p>The seismic moment tensor $\mathbf{M}$ is a mathematical representation of the motion applied to a point-sized seismic source. It is a $3 \times 3$ second-order symmetric tensor with 6 independent parameters.</p>

<figure>
<center>
<img src="https://github.com/thurinj/thurinj.github.io/blob/master/images/Workshop_Figures_2024/Moment_tensor.png?raw=true" width="400" height="400" alt="Representation of the double couple on a fault" />
<figcaption>Representation of the moment tensor $\mathbf{M}$ components in terms of linear vector dipoles and force couples. From <a href="https://www.wiley.com/en-au/An+Introduction+to+Seismology%2C+Earthquakes%2C+and+Earth+Structure-p-9780865420786">Stein &amp; Wysession (2002)</a>.</figcaption>
</center>
</figure>

<p>In the ideal case of a motion applied onto a perfect fault-plane, it can be represented by a supperposition of force-couples or dipoles (with zero net-torque):</p>

<figure>
<center>
<img src="https://github.com/thurinj/thurinj.github.io/blob/master/images/Workshop_Figures_2024/Double-couple.png?raw=true" width="400" height="140" alt="Representation of the double couple on a fault" />
<figcaption>Motion on an ideal fault plane represented as double-couples. From <a href="https://www.wiley.com/en-au/An+Introduction+to+Seismology%2C+Earthquakes%2C+and+Earth+Structure-p-9780865420786">Stein &amp; Wysession (2002)</a>.</figcaption>
</center>
</figure>

<p>Therefore, by estimating the moment tensor parameters for seismic sources that fall into the point-source approximation we can learn about their geometrical properties, and the general motion that was applied to the subsurface - generating seismic waves.</p>

<hr style="border: 1px solid   #8caba1; width: 100%;" />

<p>A moment tensor can be conveniently represented as the so-called <em>beachball</em>, which is a representation of the P-wave first motions in 3D space (binary representation of the P-waves radiation pattern). The color of the beachball reflects the direction of motion applied in different directions. It is generally white for motion pointing inward and black (or color shaded) for motion pointing outward of the unit sphere.</p>

<figure>
<center>
<img src="https://github.com/thurinj/thurinj.github.io/blob/master/images/Workshop_Figures_2024/DC.gif?raw=true" width="400" height="400" alt="Gif of a moment tensor in 3D with vector field for the motion" />
<figcaption>Vector field representation of the motion associated with double-couple point source. represented as a 'beachball'. Courtesy of <a href="https://mxrap.com/2019/07/26/moment-tensors-a-practical-guide/">Stuart Tierney</a>.</figcaption>
</center>
</figure>

<p>Most of the time, moment tensor “beachballs” are visualized as 2D diagrams. These diagrams are stereonet projections of the lower hemisphere of the unit sphere, as seen from above in a map view. This representation allows for easier interpretation and analysis, as it conveys the general motion on a fault at a glance, or the source type in the case of non double-couple sources.</p>

<figure>
<center>
<img src="https://s3.amazonaws.com/pnsn-cms-uploads/attachments/000/000/782/original/fc3a5caddf34f183f09a2382cb05136d" width="400" alt="A few key moment tensor and fault block-models." />
<figcaption>Common double couple moment tensor and associated fault block-models <a href="https://www.usgs.gov/media/images/schematic-diagram-focal-mechanism">USGS</a>.</figcaption>
</center>
</figure>

<hr style="border: 1px solid   #8caba1; width: 100%;" />

<h3 id="moment-tensor-parameterization-and-the-lune-diagram">Moment tensor parameterization and the Lune diagram</h3>

<p>Moment tensors attributes can be broadly defined as <em>source type</em> and <em>orientation</em>, which are both informed by the geometrical properties of the tensor. Given a moment tensor $\mathbf{M}$, whose eigvenvalues $(\lambda_1, \lambda_2, \lambda_3)$ satisfy $\lambda_1 \geq \lambda_2 \geq \lambda_3$, we can represent a moment tensor as its eigendecomposition:</p>

<p>$\mathbf{M} = \mathbf{U \Lambda U}^T$</p>

<p>where $\mathbf{U}$ contains the eigenvectors that satisfies $\mathbf{UU}^T = \mathbf{I}$, and $\mathbf{\Lambda}$ is the diagonal matrix containing the eigenvalue triple in descending order.</p>

<p>With this decomposition in mind, we see that $\mathbf{U}$ encodes the source orientation (the tensor’s eigenframe), and $\mathbf{\Lambda}$ encodes the source type. From this decomposition stems a very natural way of representing source-types, by projecting the tensor position on the eigenvalue lune:</p>

<figure>
<center>
<img src="https://github.com/thurinj/thurinj.github.io/blob/master/images/Workshop_Figures_2024/Eigenvalue_lune_definition.png?raw=true" width="600" height="200" alt="Geometry of the eigenvalue lune" />
<figcaption>Definition of the eigenvalue lune as a set of ordered and normalized eigenvalues. From <a href="https://drive.google.com/file/d/1-B37EEHRp-ZpycFYVGj3TRUFOy3H0VoJ/view">Tape &amp; Tape 2019</a>.</figcaption>
</center>
</figure>

<p>In the following example, we show for 4 fixed eigenframes, how the moment-tensor source type changes as a function of $\mathbf{\Lambda}$ only:</p>

<figure>
<center>
<img src="https://github.com/thurinj/thurinj.github.io/blob/master/images/Workshop_Figures_2024/4Lunes.png?raw=true" width="600" height="320" alt="4 moment tensor lunes" />
<figcaption>Moment-tensor source type plot for a set of 4 fixed source orientations. For a fixed orientation, the location on the eigenvalue lune controls the source type. From <a href="https://sites.google.com/alaska.edu/carltape/home/research/beachball_gallery?authuser=0">Carl Tape</a> figure gallery.</figcaption>
</center>
</figure>

<p>In <code class="language-plaintext highlighter-rouge">MTUQ</code>, we adopt a moment-tensor parameterization that exploits the geometrical properties of the moment tensor so that we explore the moment-tensor space in a uniform manner. Instead of exploring the 6 parameters of the moment tensor directly, we will express moment tensors with the following set of coordinates:</p>

<ul>
  <li>$γ, δ$ for the eigenvalue triple, $γ$ being the lune longitude and $δ$ the lune latitude,</li>
  <li>$κ, σ, θ$ for the orientaion, where $κ$, $σ$, $θ$ are for strike, dip and, slip angles respectively.</li>
  <li>$ρ$ gives the scalar seismic moment $M_0 = ρ \sqrt{2}^{-1}$</li>
</ul>

<p>such that we have $\mathbf{M}(ρ, γ, δ, κ, σ, θ)$.</p>

<p>For more details, you can read <a href="https://drive.google.com/file/d/1s41vK795cncM_IeTI617Y3GaZ6mVbQhp/view">Tape &amp; Tape (2015)</a>.</p>

<h3 id="moment-tensor-inversion">Moment tensor inversion</h3>
<p>In <code class="language-plaintext highlighter-rouge">MTUQ</code> we solve the moment tensor inversion problem by finding the source that minimizes the following cost function:</p>

<p>$C(\mathbf{M}) = \sum_{r=1}^{N} \sum_{i=1}^{3} \int_0^T \left[d_i^r(t,\mathbf{M}) - d_i^{r,obs}(t)\right]^2 dt$</p>

<p>which translates to the sum of least-squares waveform difference between the synthetic waveforms $d(t)$ and the observed data $d^{obs}(t)$ for N stations with 3 components.</p>

<p>In order to speed up the optimization process, we rely on a set of pre-computed Green’s functions $\mathbf{G}^r$ between each source-receiver pair $r$, such that we can recast the problem as</p>

<p>$C(\mathbf{M}) = \sum_{r=1}^{N} \sum_{i=1}^{3} \int_0^T \left[\mathbf{G}_i^r \mathbf{m} - d_i^{r,obs}\right]^2 dt$</p>

<p>where the time dependency has been dropped for convenience, and $\mathbf{m}$ is a vector containing the 6 independent parameters of the Moment tensor $\mathbf{M}$. Using pre-computed Green’s functions allows the rapid evaluation of the residual term, which we simplify as:</p>

<p>$\mathbf{r} = \mathbf{Gm} - \mathbf{d}$</p>

<p>which is less expensive than explicitly modeling the waveform for each source in a given earth model.</p>

<h3 id="the-cut-and-paste-method">The Cut-and-paste method</h3>

<p>The algorithm underlying <code class="language-plaintext highlighter-rouge">MTUQ</code> is known as the “cut-and-paste” (CAP)method of <a href="https://doi.org/10.1785/BSSA0860051634"> Zhu &amp; Helmberger (1996)</a>.</p>

<p>The main goal of the CAP method is to mitigate the effects of 3-D velocity structures that are unaccounted for with a 1D reference model (or an inexact 3D model).</p>

<p>It does so by splitting all the data into specific window groups (typically P-waves and Surface waves) and allowing a time-shift window for the synthetics. Within this allowable time-shift window, a cross-correlation is performed between $d_{syn}$ and $d_{obs}$, and the residual is computed only for the maximum cross-correlation value (when signals are supposedly aligned).</p>

<figure>
<center>
<img src="https://github.com/thurinj/thurinj.github.io/blob/master/images/Workshop_Figures_2024/CAP.gif?raw=true" width="600" height="320" alt="Illustration of the cut and paste method." />
<figcaption>Illustration of the cut and paste method.</figcaption>
</center>
</figure>

<p>We do this for each data group (P, Rayleigh, Love waves), for each station and each component (Z-R-T), and return one misfit value corresponding to the maximum cross-correlation value.</p>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[This introduction is a brief overview of the moment tensor inversion problem as handled by the MTUQ package. It is directly copied from the Google Colab notebook that I used for the SCOPED 2024 Workshop session on moment tensor inversion. The notebook can be found here.]]></summary></entry><entry><title type="html">Inverse problems</title><link href="http://localhost:4000/jekyll/update/2024/07/20/Inverse_Problems.html" rel="alternate" type="text/html" title="Inverse problems" /><published>2024-07-20T00:43:14+10:00</published><updated>2024-07-20T00:43:14+10:00</updated><id>http://localhost:4000/jekyll/update/2024/07/20/Inverse_Problems</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/07/20/Inverse_Problems.html"><![CDATA[<h1 id="inverse-problems">Inverse problems</h1>

<p>Here is a very general introduction to inverse problems, which I hope will be helpful to you. Through my undergrad and grad studies, I have been lucky to find plenty of excellent resources on the topic online, and I hope to contribute to it with my own words here, hoping to pass the torch along.</p>

<h3 id="inverse">Inverse?</h3>
<p>What is an <strong>inverse problem</strong> in the first place? In a nutshell, an inverse problem is a problem driven by observations. It is like a game of Clue, where the goal is to infer, guess, or estimate <em>what</em> generated those observations. This class of problems is ubiquitous in science and engineering. It can be found in a wide range of fields, from earth sciences, medical imaging, and computer science to logistics, finance, and economics, to name a few. As their name suggests, inverse problems are somehow linked to what we call <strong>forward problems</strong>. In a forward problem, we know the cause and we want to predict the effect. Given a model (a set of rules driving a system), some initial conditions, we can predict the outcome. The inverse problem works backward: given the outcome, and knowing some of the rules, we want to infer the initial conditions or the model that produced it.</p>

<p>One great visual and oversimplistic analogy would be to compare the forward and inverse problems in the context of Baking a cake:</p>

<ul>
  <li>
    <p><em>Forward Problem</em>: You have access to a recipe (the model), a set of ingredients and baking temperature (initial conditions). You follow the recipe step by step to bake a cake (the outcome). This is straightforward—given the instructions and ingredients, you can predict the final product - which if you don’t mess up, should be a delicious cake.</p>
  </li>
  <li>
    <p><em>Inverse Problem</em>: Now, suppose you come across a beautifully baked cake but without knowing the recipe. You want to reverse-engineer the process: what ingredients were used, and in what quantities? What steps were followed to achieve that result? This is much more challenging because multiple recipes could potentially produce similar cakes. You enter the territory of <em>uncertainty</em>, <em>educated guesses</em>, <em>approximations</em> and <em>probabilities</em>.</p>
  </li>
</ul>

<p>And how would you know if you have guessed right? You would need to bake the cake yourself and compare the results with some sort of criteria (we will talk soon about a <em>metric</em>). This is where <strong>data</strong> comes into play. In the context of inverse problems, data is the key to validate your guesses and refine your model. The more data you have, the more constraints you can put on your model, and the more accurate your predictions will be.</p>

<h3 id="optimization">Optimization</h3>

<p>The classical way of tackling inverse problem, is to frame it as a numerical optimization procedure. Essentially, we’re looking out for the best possible solution that aligns with our observations and adheres to the underlying model (i.e., the perfect copy of the Chef’s cake you are trying to reproduce). This is where the concept of <strong>misfit function</strong>, or <strong>objective function</strong> (which is our <strong>metric</strong>) comes into play.</p>

<p>To make this clear, let’s define our objective function as the following minimization problem:</p>

\[\min_{\mathbf{x}} \left\| \mathbf{A}\mathbf{x} - \mathbf{b} \right\|^2\]

<p>where:</p>
<ul>
  <li>$\mathbf{A}$ is the forward operator</li>
  <li>$\mathbf{x}$ is the parameter vector we want to estimate</li>
  <li>$\mathbf{b}$ is the observed data</li>
  <li>$\left|\left| \cdot \right|\right|^2$ is the norm that measures the discrepancy between predicted and observed data.</li>
</ul>

<p>Back to baking! In our analogy, the forward model $\mathbf{A}$ represents the recipe. It dictates how different ingredients (the parameters of our inverse problem) combine under certain conditions (like baking temperature) to produce the final cake. Mathematically, $\mathbf{A}$ transforms the parameter vector $\mathbf{x}$ (ingredient quantities) into the predicted outcome $\mathbf{Ax}$  (cake characteristics).</p>

<p>The parameter vector $\mathbf{x}$ is the set of all the quantities of each ingredient used in the cake. For example $\mathbf{x} = \left[x_1, x_2, x_3, x_4\right]$ could represent the amounts of flour, sugar, eggs, etc., needed to bake the cake.</p>

<p>The observed data $\mathbf{b}$ corresponds to the characteristics of the final cake you have—its taste, obviously, but it could also be texture, color, and other attributes. These are the outcomes you observe and wish to match by adjusting the ingredient quantities. You see from here that the observation you make is crucial in defining the complexity of the problem. If you only have a vague idea of what the cake should look like, you might end up with a very different recipe than the original one, due to the lack of constraints you impose on your optimization problem.</p>

<h3 id="putting-it-all-together">Putting It All Together</h3>

<p>In the inverse problem scenario, you’re given $\mathbf{b}$ (the Chef’s cake) and $\mathbf{A}$ (the general idea of the recipe), but you need to determine $\mathbf{x}$ (the specific ingredient quantities) that would produce a cake matching your observations. The optimization task involves finding the $\mathbf{x}$ that minimizes the difference between $\mathbf{Ax}$ (the predicted cake characteristics based on your guessed ingredients) and $\mathbf{b}$ (the actual cake you have).</p>

<p>Replace $\mathbf{A}$ by the wave equation, and $\mathbf{b}$ with a seismogram, and you will quickly be looking at a seismic tomography problem, where the goal is now to infer the Earth’s structure ($\mathbf{x}$) from seismic waves traveling through it.</p>

<h2 id="global-vs-local-optimization-methods">Global vs Local Optimization methods</h2>

<p>When solving inverse problems, one of the critical decisions is choosing the right optimization method. Broadly, these methods fall into two categories: <em>global optimization</em> and <em>local optimization</em>. Understanding the differences between these approaches is essential for selecting the most appropriate method for a given problem, as unfortunately, there is no such thing as a one-size-fits-all optimization algorithm.</p>

<h3 id="global-optimization">Global optimization</h3>
<p>Global optimization methods aim to find the best possible solution within the entire search space (also “misfit space” or “solution space”). They are designed to avoid getting trapped in local optima—solutions that are better than their immediate neighbors but not the best overall. If there is such a thing as the best overall solution for a problem, global optimization is designed to find it. They are generally quite robust but also computationally demanding, as they need to explore a large number of solutions to ensure they have found the best one. They are usually not recommended when the number of parameters to estimate is “large.” How large exactly is subject to interpretation and might vary from one problem to another, but in general, global optimization methods can become computationally prohibitive as the number of parameters grows (that’s the <a href="https://en.wikipedia.org/wiki/Curse_of_dimensionality">curse of dimensionality</a>), often becoming impractical beyond a few dozen parameters depending on the specific method and problem structure. Some popular methods you might have heard of are:</p>

<ol>
  <li><a href="https://en.wikipedia.org/wiki/Simulated_annealing">Simulated annealing</a></li>
  <li><a href="https://en.wikipedia.org/wiki/Particle_swarm_optimization">Particle swarm optimization</a></li>
  <li><a href="https://en.wikipedia.org/wiki/Genetic_algorithm">Genetic algorithms</a></li>
  <li><a href="https://en.wikipedia.org/wiki/Brute-force_search">Grid search</a> (or exhaustive search, bruteforce search)</li>
</ol>

<p>Some of these methods (1-3) are inspired by natural phenomena or specific patterns in nature and fall into the category of <strong>metaheuristic</strong> optimization methods, which I hope to cover in a future post (more specifically, I would like to discuss some of the methods from the brilliant <a href="https://github.com/fcampelo/EC-Bestiary">Evolutionary Computation Bestiary</a> that <a href="https://github.com/fcampelo">Felipe Campelo</a> has put together). The last method (4) is more general and can be applied to a wide range of problems, but suffers from the exponential growth in possible combinations.</p>

<p><strong>Baking analogy!</strong> Applied to our cake-copy problem, a global optimization method would require you to try out many  combinations of available ingredients to find the best recipe that matches your observations. While it might not be a big deal if you could instantly bake a cake and taste it, it is a totally different story if you have to work for several hours to bake each cake: Due to the generally high number of estimates to make, global optimization methods are generally not recommended if the forward problem is very computationally expensive.</p>

<div style="height: 0px;"></div>

<h3 id="local-optimization">Local optimization</h3>
<p>Local optimization methods, on the other hand, are designed to find the best solution in the immediate vicinity of the starting point. Essentially, they are operating within a limited portion of the search space. They are generally faster than global optimization methods, but they are also more likely to get stuck in local optima. If the best overall solution is not in the vicinity of your search region (your first guess), you probably won’t be able to find it with a local optimization method. While they are not able to guarantee finding the global minimum, most of them have the advantage of being able to exploit the local structure of the search space (like slope and curvature of the objective function), which can lead to faster convergence. Some popular methods you might have heard of are:</p>

<ol>
  <li><a href="https://en.wikipedia.org/wiki/Gradient_descent">Gradient descent</a></li>
  <li><a href="https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization">Newton’s method</a></li>
  <li><a href="https://en.wikipedia.org/wiki/Levenberg%E2%80%93Marquardt_algorithm">Levenberg-Marquardt</a></li>
  <li><a href="https://en.wikipedia.org/wiki/Conjugate_gradient_method">Conjugate gradient</a></li>
  <li><a href="https://en.wikipedia.org/wiki/Quasi-Newton_method">Quasi-Newton methods</a></li>
</ol>

<p>When using the gradient of the objective function to guide the search, these methods are called <strong>gradient-based</strong> optimization methods (or first order methods). When they use the Hessian (second derivative of the objective function), they are called <strong>second-order</strong> methods. The latter are generally more computationally expensive, but they can provide more information about the local structure of the search space, which can lead to faster convergence, and potentially uncertainty estimation by estimating the local curvature of the misfit function in the vicinity of the solution.</p>

<p><strong>Baking analogy!</strong> Applied to our cake-copy problem, a local optimization method would require you to adjust the quantities of ingredients in your recipe based on the taste of the cake you have just baked. You would then taste the cake again and adjust the recipe again, and so on, until you are satisfied with the result. This is a much faster process than trying out a huge number of ingredients combinations, but you might end up with a cake that is not as good as the one you could have made with a global optimization method. If your initial cake (i.e., the first guess that defines the local search region) is not close to the cake you are trying to copy, you will probably end up with a suboptimal recipe.</p>

<hr style="border: 1px solid   #8caba1; width: 100%;" />

<h2 id="conclusion">Conclusion</h2>

<p>If you are not familiar with inverse problems or optimization methods, I hope this very general introduction has given you a taste of what they are about. The field of inverse problems is vast, and I’ve only scratched the surface here. I have left out misfit functions, regularization, machine learning, uncertainty quantification, Bayesian inference, and many more topics.</p>

<p>For beginners who might be reading this, I recommend starting with a simple problem and trying to solve it with a local optimization method. This will give you a good understanding of the basic concepts and challenges of inverse problems. Once you are comfortable with the basics, you can start exploring more advanced topics and methods.</p>

<p>And as for some useful reference, here are some (mostly free) resources you might want to check out:</p>

<ul>
  <li><a href="https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python">Kalman and Bayesian filter in Python</a>: An e-book written by Roger R. Labbe. While strictly speaking, the book is about Data Assimilation, it essentially is a different spin on classical inverse problems (it is basically inverse problems for dynamic systems). It is a fantastic introduction to the topic, with lots of very concrete examples written in Python.</li>
  <li><a href="https://algorithmsbook.com/optimization/">Algorithms for Optimization</a>: This book is published by MIT press and available freely online. It covers a wide range of topic, but mostly focuses on the algorithm to solve optimization problems. It is a great resource if you are interested in the numerical side of inverse problems.</li>
  <li><a href="https://www.geologie.ens.fr/~jolivet/Research_files/Tarantola.pdf">Inverse Problem Theory</a>: written by the late Albert Tarantola, this is THE reference book for inverse problem theory in geophysics. It covers practically everything. While I would not necessarily recommend it as a first read for a beginner, it is a must have reference. And for a great piece of Earth’s science and web history combined, here is Albert Tarantola’s <a href="http://web.archive.org/web/20230601164217/https://www.ipgp.fr/~tarantola/">original web page</a> archived on the <a href="http://web.archive.org/">Internet Archive</a> wayback machine.</li>
  <li><a href="https://isharifi.ir/teaching/2019/IoT/%5BDan_Simon%5D_Optimal_State_Estimation_Kalman,_H_In%28BookFi%29.pdf">Optimal State Estimation, Kalman, Hoo, and Nonlinear Approaches</a>: A book by Dan Simon. While it is mostly about Kalman filtering, it is also a great introduction to the topic of statistical filtering / parameter estimation. Inverse problems are, after all, very reminiscent of estimation theory and filtering.</li>
  <li><a href="https://www.math.uci.edu/~qnie/Publications/NumericalOptimization.pdf">Numerical Optimization</a>: A book by Jorge Nocedal and Stephen J. Wright. This one is also focused on the numerical aspect of optimization. It gives an exhaustive overview of the practical aspects of optimization, and the sections on 2nd order optimization are particularly brilliant. A must if you want to implement your own optimization algorithms.</li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Inverse problems]]></summary></entry></feed>